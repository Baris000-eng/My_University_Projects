{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtiXkElr0IVV",
        "outputId": "b143e5fe-9f7b-42b9-b88d-88edad4266fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "---------------------------------BEGINNING OF TRAINING THE CNN MODEL----------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 242MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------------------------------------------------------------------------\n",
            "For the training dataset: \n",
            "\n",
            "\n",
            "The value of overall accuracy is the following: 0.9615384615384616\n",
            "\n",
            "The values of class accuracies are the following: [0.9487179487179487, 0.9838709677419355, 0.9310344827586207]\n",
            "\n",
            "The class accuracy for the Class 1 is the following: 0.9487179487179487\n",
            "The class accuracy for the Class 2 is the following: 0.9838709677419355\n",
            "The class accuracy for the Class 3 is the following: 0.9310344827586207\n",
            "-------------------------------------------------------------------------------------------\n",
            "\n",
            "-------------------------------------------------------------------------------------------\n",
            "For the validation dataset: \n",
            "\n",
            "\n",
            "The value of overall accuracy is the following: 0.8763440860215054\n",
            "\n",
            "The values of class accuracies are the following: [0.7666666666666667, 0.9659090909090909, 0.8421052631578947]\n",
            "\n",
            "The class accuracy for the Class 1 is the following: 0.7666666666666667\n",
            "The class accuracy for the Class 2 is the following: 0.9659090909090909\n",
            "The class accuracy for the Class 3 is the following: 0.8421052631578947\n",
            "-------------------------------------------------------------------------------------------\n",
            "\n",
            "-------------------------------------------------------------------------------------------\n",
            "For the test dataset: \n",
            "\n",
            "\n",
            "The value of overall accuracy is the following: 0.7242424242424242\n",
            "\n",
            "The values of class accuracies are the following: [0.5740740740740741, 0.8827586206896552, 0.6363636363636364]\n",
            "\n",
            "The class accuracy for the Class 1 is the following: 0.5740740740740741\n",
            "The class accuracy for the Class 2 is the following: 0.8827586206896552\n",
            "The class accuracy for the Class 3 is the following: 0.6363636363636364\n",
            "-------------------------------------------------------------------------------------------\n",
            "\n",
            "-----------------------------------END OF TRAINING THE CNN MODEL----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "from torchvision.datasets.folder import DatasetFolder\n",
        "from torchvision.datasets.folder import default_loader\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision.transforms.functional import normalize\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.transforms.functional import normalize\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from google.colab import drive\n",
        "\n",
        "######################################### Barış Kaplan (0069054) ##########################################################################\n",
        "######################################### Canberk Eker (0060233) ##########################################################################\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_directory = '/content/drive/MyDrive/dataset/'\n",
        "os.chdir(data_directory)\n",
        "\n",
        "\n",
        "print(\"---------------------------------BEGINNING OF TRAINING THE CNN MODEL----------------------------------------------------------\")\n",
        "print()\n",
        "transforms_of_datas = {\n",
        "    'training': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ############### This applies input normalization #############\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ############### This applies input normalization #############\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ############### This applies input normalization #############\n",
        "    ]),\n",
        "}\n",
        "\n",
        "class MyCustomDefinedDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "        lst_of_files = list()\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                lst_of_files.append(file)\n",
        "        self.file_list = lst_of_files\n",
        "        self.labels = self.load_labels()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_name = self.file_list[index]\n",
        "        image_path = os.path.join(self.root_dir, file_name)\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except (FileNotFoundError, OSError):\n",
        "            ####### Skip the invalid image files ########\n",
        "            return self.__getitem__((index + 1) % len(self))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[index]\n",
        "        return image, label\n",
        "\n",
        "    def load_labels(self):\n",
        "        label_path = os.path.join(self.root_dir, '..', self.label_file)\n",
        "        with open(label_path, 'r') as f:\n",
        "            labels = [int(line.strip()) - 1 for line in f]  \n",
        "        return list(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "\n",
        "full_training_dataset = MyCustomDefinedDataset(os.path.join(data_directory, 'training'), 'training_labels.txt',\n",
        "                                      transform=transforms_of_datas['training'])\n",
        "\n",
        "### Split the full training dataset into training and validation portions ###\n",
        "train_size = int(0.70 * len(full_training_dataset))  ########### assigning 70 percent of full training dataset as training portion ###############\n",
        "valid_size = len(full_training_dataset) - train_size  ########### assigning 30 percent of full training dataset as validation portion ###############\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(full_training_dataset, [train_size, valid_size]) ####### train and validation dataset splitting ######\n",
        "\n",
        "image_datasets = {\n",
        "    'training': train_dataset,\n",
        "    'validation': valid_dataset,\n",
        "    'test': MyCustomDefinedDataset(os.path.join(data_directory, 'test'), 'test_labels.txt',\n",
        "                                   transform=transforms_of_datas['test'])\n",
        "}\n",
        "\n",
        "\n",
        "#####this function assigns equal weights to each image in each dataset type#####\n",
        "def do_not_handle_class_imbalance_and_assign_equal_weights(imgs, total_class_number):\n",
        "    wgh_values_not_balanced = {}\n",
        "    data_types_lst = ['training', 'validation', 'test']\n",
        "    for data_typ in data_types_lst:\n",
        "        length_of_img_lst = len(imgs[data_typ])\n",
        "        values_of_weights = [1.0] * length_of_img_lst  ####### Assigning equal weight of 1.0 to all images ############\n",
        "        wgh_values_not_balanced[data_typ] = values_of_weights\n",
        "    print(wgh_values_not_balanced)\n",
        "    return wgh_values_not_balanced\n",
        "\n",
        "\n",
        "#####this function handles the class imbalance problem by assigning lower weights to overrepresented images and higher weights to underrepresented ones#####\n",
        "def handle_class_imbalance_and_assign_weights(imgs, total_class_number):\n",
        "    lst_cntr = []\n",
        "    for w in range(0, total_class_number):\n",
        "        lst_cntr.append(0)\n",
        "    wgh_values_balanced = {}\n",
        "    data_types_lst = ['training', 'validation','test']\n",
        "    for data_typ in data_types_lst:\n",
        "        for img_name in imgs[data_typ]:\n",
        "          value_of_label = img_name[1]\n",
        "          lst_cntr[value_of_label] = lst_cntr[value_of_label] + 1\n",
        "        weight_vals_of_cls = list()\n",
        "        for d in range(0, total_class_number):\n",
        "          count_of_all = sum(lst_cntr)\n",
        "          weight_val = float(count_of_all) / lst_cntr[d]\n",
        "          weight_vals_of_cls.append(weight_val)\n",
        "        values_of_weights = []\n",
        "        length_of_img_lst = len(imgs[data_typ])\n",
        "        for q in range(0, length_of_img_lst):\n",
        "          values_of_weights.append(0)\n",
        "        for loc, val in enumerate(imgs[data_typ]):\n",
        "          label_value = val[1]\n",
        "          values_of_weights[loc] = weight_vals_of_cls[label_value] ############### weight value assignment to each image ###########\n",
        "        wgh_values_balanced[data_typ] = values_of_weights\n",
        "    return wgh_values_balanced\n",
        "\n",
        "\n",
        "weight_values_with_balance = handle_class_imbalance_and_assign_weights(image_datasets, 3)\n",
        "all_data_types = ['training', 'validation', 'test']\n",
        "weighted_random_sampler = {k: torch.utils.data.sampler.WeightedRandomSampler(weight_values_with_balance[k], len(weight_values_with_balance[k])) for k in all_data_types}\n",
        "vars_dataload = {l: torch.utils.data.DataLoader(image_datasets[l],batch_size = 4, shuffle = True, num_workers = 4) for l in all_data_types}\n",
        "device_in_torch = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "convolutional_model = models.alexnet(pretrained = True)\n",
        "for param in convolutional_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "\n",
        "size_of_input = convolutional_model.classifier[6].in_features\n",
        "convolutional_model.classifier[6] = nn.Linear(size_of_input, 3)\n",
        "convolutional_model = convolutional_model.to(device_in_torch) \n",
        "criterion = nn.CrossEntropyLoss() ####### cross entropy loss function definition #########\n",
        "convolutional_model_optimizer = optim.SGD(convolutional_model.parameters(), lr=0.0015, momentum=0.75)\n",
        "exp_learning_rate_scheduler = lr_scheduler.StepLR(convolutional_model_optimizer, step_size=4, gamma=0.10)\n",
        "convolutional_model.eval\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_correct_guesses = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in vars_dataload['training']:\n",
        "            inputs = inputs.to(device_in_torch)\n",
        "            labels = labels.to(device_in_torch)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        number_of_corrects = 0\n",
        "        for inputs, labels in vars_dataload['validation']:\n",
        "            inputs = inputs.to(device_in_torch)\n",
        "            labels = labels.to(device_in_torch)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                pred_equal_label_sum = torch.sum(preds == labels.data)\n",
        "                number_of_corrects = number_of_corrects + pred_equal_label_sum\n",
        "            if number_of_corrects > best_correct_guesses:\n",
        "                best_correct_guesses = number_of_corrects\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "trained_convolutional_model = train_model(convolutional_model, criterion, convolutional_model_optimizer,exp_learning_rate_scheduler, num_epochs=100)\n",
        "data_types_lst = ['training', 'validation', 'test']\n",
        "c1 = 0\n",
        "i1 = 0\n",
        "c2 = 0\n",
        "i2 = 0\n",
        "c3 = 0\n",
        "i3 = 0\n",
        "with torch.no_grad():\n",
        "  for data_type in data_types_lst:\n",
        "    for images_inputted, classes_of_images in vars_dataload[data_type]:\n",
        "      inputted_to_device = images_inputted.to(device_in_torch)\n",
        "      corresponding_classes_to_device = classes_of_images.to(device_in_torch)\n",
        "      outcome_values = trained_convolutional_model(inputted_to_device)\n",
        "      test_values, values_of_predictions = torch.max(outcome_values, 1)\n",
        "      for lbl_of_class, value_of_prediction in zip(corresponding_classes_to_device, values_of_predictions):\n",
        "        if int(lbl_of_class) == 0:\n",
        "          if int(value_of_prediction) == 0:\n",
        "            c1 = c1 + 1\n",
        "          else:\n",
        "            i1 = i1 + 1\n",
        "        if int(lbl_of_class) == 1:\n",
        "          if int(value_of_prediction) == 1:\n",
        "            c2 = c2 + 1\n",
        "          else:\n",
        "            i2 = i2 + 1\n",
        "        if int(lbl_of_class) == 2:\n",
        "          if int(value_of_prediction) == 2:\n",
        "            c3 = c3 + 1\n",
        "          else:\n",
        "            i3 = i3 + 1\n",
        "\n",
        "    print()\n",
        "    print(\"-------------------------------------------------------------------------------------------\")\n",
        "    print(\"For the \" + str(data_type) + \" dataset: \")\n",
        "    print()\n",
        "    value_of_ovr_acc = (c1 + c2 + c3) / (c1 + i1 + c2 + i2 + c3 + i3)\n",
        "    print()\n",
        "    print(\"The value of overall accuracy is the following: \" + str(value_of_ovr_acc) + \"\")\n",
        "    print()\n",
        "    values_of_class_accuracies = [c1/(c1+i1), c2/(c2+i2), c3/(c3+i3)]\n",
        "    print(\"The values of class accuracies are the following: \" + str(values_of_class_accuracies) + \"\")\n",
        "    print()\n",
        "    for i, class_accuracy in enumerate(values_of_class_accuracies):\n",
        "        print(\"The class accuracy for the Class \" + str(i + 1) + \" is the following: \" + str(class_accuracy) + \"\")\n",
        "    print(\"-------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------END OF TRAINING THE CNN MODEL----------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}